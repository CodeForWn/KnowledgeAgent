# -*- coding: utf-8 -*-
from flask import Flask, request, jsonify, send_from_directory, Response, stream_with_context
from flask_cors import CORS
from transformers import AutoTokenizer, AutoModel
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import json
import os
import threading
import queue
import urllib3
import logging
import requests
from logging.handlers import RotatingFileHandler
import sys
import re
import uuid
import jieba.posseg as pseg
from FlagEmbedding import FlagReranker
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from config.KMC_config import Config
from ElasticSearch.KMC_ES import ElasticSearchHandler
from File_manager.KMC_FileHandler import FileManager
from LLM.KMC_LLM import LargeModelAPIService
from Prompt.KMC_Prompt import PromptBuilder
import types
import time
import os


class NoRequestStatusFilter(logging.Filter):
    def filter(self, record):
        return "/api/request_status" not in record.getMessage()


# ä¸º werkzeug æ·»åŠ è¿‡æ»¤å™¨
werkzeug_log = logging.getLogger('werkzeug')
werkzeug_log.addFilter(NoRequestStatusFilter())


urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
app = Flask(__name__)
CORS(app)
# åŠ è½½é…ç½®
# ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šç¯å¢ƒå¹¶åŠ è½½é…ç½®
config = Config(env='production')
config.load_config()  # æŒ‡å®šé…ç½®æ–‡ä»¶çš„è·¯å¾„
config.load_predefined_qa()
logger = config.logger
record_path = config.record_path
backend_notify_api = config.external_api_backend_notify
# åˆ›å»º FileManager å®ä¾‹
file_manager = FileManager(config)
# åˆ›å»ºElasticSearchHandlerå®ä¾‹
es_handler = ElasticSearchHandler(config)
large_model_service = LargeModelAPIService(config)
prompt_builder = PromptBuilder(config)
rerank_model_path = config.rerank_model_path

# åˆ›å»ºé˜Ÿåˆ—
file_queue = queue.Queue()
# åˆ›å»ºå…¨å±€é”ï¼Œç¡®ä¿ä¸€æ¬¡åªæœ‰ä¸€ä¸ªæ–‡ä»¶å¤„ç†
file_processing_lock = threading.Lock()
index_lock = threading.Lock()
logger.info('æœåŠ¡å¯åŠ¨ä¸­ã€‚ã€‚ã€‚')

# åˆå§‹åŒ–è¯·æ±‚çŠ¶æ€å­—å…¸
request_status = {}
request_lock = threading.Lock()


def cleanup_request_status():
    with request_lock:
        current_time = time.time()
        for req_id in list(request_status.keys()):
            if current_time - request_status[req_id]['start_time'] > 120:  # è¶…è¿‡ä¸€å°æ—¶çš„è®°å½•
                del request_status[req_id]


@app.before_request
def before_request():
    cleanup_request_status()
    request_id = str(uuid.uuid4())  # ç”Ÿæˆå”¯ä¸€è¯·æ±‚ID
    request.environ['REQUEST_ID'] = request_id  # å°†è¯·æ±‚IDå­˜å‚¨åœ¨è¯·æ±‚ç¯å¢ƒä¸­
    with request_lock:
        request_status[request_id] = {
            "start_time": time.time(),
            "status": "processing",
            "url": request.url
        }
    # logger.info(f"å¼€å§‹å¤„ç†è¯·æ±‚: {request_id} for {request.url}")


@app.after_request
def after_request(response):
    request_id = request.environ.get('REQUEST_ID')
    with request_lock:
        if request_id in request_status:
            # å¦‚æœè¯·æ±‚ä»ç„¶å¤„äº "processing" çŠ¶æ€ï¼Œåˆ™å°†å…¶æ›´æ–°ä¸º "completed"
            if request_status[request_id]["status"] == "processing":
                request_status[request_id]["end_time"] = time.time()
                request_status[request_id]["status"] = "completed"
                # æ—¥å¿—è®°å½•ï¼ˆå¯é€‰ï¼‰
                # logger.info(f"å®Œæˆè¯·æ±‚: {request_id} for {request.url}")
    return response


@app.teardown_request
def teardown_request(exception):
    request_id = request.environ.get('REQUEST_ID')
    with request_lock:
        # æ£€æŸ¥è¯·æ±‚çŠ¶æ€æ˜¯å¦ä»ç„¶æ˜¯ "processing"ï¼Œä»¥é˜²æ­¢é‡å¤æ›´æ–°
        if request_id in request_status and request_status[request_id]["status"] == "processing":
            request_status[request_id]["end_time"] = time.time()
            request_status[request_id]["status"] = "failed" if exception else "completed"
            # æ—¥å¿—è®°å½•å¤±è´¥çš„è¯·æ±‚
            logger.info(f"è¯·æ±‚å¤±è´¥: {request_id} for {request.url} due to {exception}")


@app.route('/favicon.ico')
def favicon():
    return send_from_directory(os.path.join(app.root_path, 'static'), 'favicon.ico', mimetype='image/vnd.microsoft.icon')


@app.route('/api/request_status', methods=['GET'])
def get_request_status():
    with request_lock:
        # è¿‡æ»¤ active_requests ä¸­çš„ /api/request_status è¯·æ±‚
        active_requests = {k: v for k, v in request_status.items() if v['status'] == 'processing' and '/api/get_answer_stream' in v['url']}
        # è¿‡æ»¤ completed_requests ä¸­çš„ /api/request_status è¯·æ±‚
        completed_requests = {k: v for k, v in request_status.items() if v['status'] == 'completed' and '/api/get_answer_stream' in v['url']}
    return jsonify({
        'active_requests': active_requests,
        'completed_requests': completed_requests
    }), 200


@app.route('/api/monitor')
def serve_monitor_page():
    return send_from_directory(os.path.join(app.root_path, 'static'), 'request_monitor.html')


def generate_assistant_id():
    # ç”Ÿæˆä¸€ä¸ªéšæœºçš„UUIDå¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    return str(uuid.uuid4())


@app.route('/')
def hello_world():
    return 'Hello, World!'


def notify_backend(file_id, result, failure_reason=None):
    """é€šçŸ¥åç«¯æ¥å£å¤„ç†ç»“æœ"""
    url = backend_notify_api  # æ›´æ–°åçš„åç«¯æ¥å£URL
    headers = {'token': file_id}
    payload = {
        'id': file_id,
        'result': result
    }
    if failure_reason:
        payload['failureReason'] = failure_reason

    response = requests.post(url, json=payload, headers=headers)
    logger.info("åç«¯æ¥å£è¿”å›çŠ¶æ€ç ï¼š%s", response.status_code)
    return response.status_code


def pull_file_data():
    # æ¨¡æ‹Ÿä»æœåŠ¡å™¨è·å–æ–‡ä»¶æ•°æ®
    return []

processed_files = set()  # ç”¨äºè¿½è¸ªå·²ç»å¤„ç†çš„æ–‡ä»¶
def _push(file_data):
    global file_queue, processed_files
    if file_data['file_id'] not in processed_files:
        file_queue.put(file_data)
        processed_files.add(file_data['file_id'])
    else:
        logger.info(f"æ–‡ä»¶ {file_data['file_id']} å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡")


def _process_file_data(data):
    try:
        with file_processing_lock:
            with app.app_context():
                user_id = data.get('user_id')
                assistant_id = data.get('assistant_id')
                file_id = data.get('file_id')
                file_name = data.get('file_name')
                download_path = data.get('download_path')
                tenant_id = data.get('tenant_id')
                tag = data.get('tag')
                createTime = data.get('createTime')

                logger.info("å¼€å§‹ä¸‹è½½æ–‡ä»¶å¹¶å¤„ç†: %sï¼Œæ–‡ä»¶è·¯å¾„ï¼š%s", file_name, download_path)
                file_path = file_manager.download_pdf(download_path, file_id)
                doc_list = file_manager.process_pdf_file(file_path, file_name)

                if not doc_list:
                    notify_backend(file_id, "FAILURE", "æœªèƒ½æˆåŠŸå¤„ç†PDFæ–‡ä»¶")
                    logger.error("æœªèƒ½æˆåŠŸå¤„ç†PDFæ–‡ä»¶: %s", file_id)
                    return

                index_name = assistant_id
                try:
                    createTime_int = int(createTime)
                except ValueError as ve:
                    logger.error(f"Invalid createTime value: {createTime}. Error: {ve}")
                    es_handler.notify_backend(file_id, "FAILURE", f"Invalid createTime value: {createTime}")
                    return

                es_handler.create_index(index_name, doc_list, user_id, assistant_id, file_id, file_name, tenant_id, download_path, tag, createTime_int)
                es_handler.notify_backend(file_id, "SUCCESS")
                logger.info("æ–‡ä»¶å¤„ç†å®Œæˆå¹¶åˆ›å»ºç´¢å¼•: %s", file_name)

            # åœ¨æ–‡ä»¶å¤„ç†å®Œæˆåæ·»åŠ  3 ç§’é—´éš”
            time.sleep(3)

    except Exception as e:
        logger.error("å¤„ç†æ–‡ä»¶æ•°æ®å¤±è´¥: %s", e)
        es_handler.notify_backend(file_id, "FAILURE", str(e))


def _thread_index_func(isFirst):
    while True:
        try:
            _index_func(isFirst)
        except Exception as e:
            print("ç´¢å¼•å¤„ç†å¤±è´¥:", e)


def _index_func(isFirst):
    global file_queue
    try:
        file_data = file_queue.get(timeout=5)
        logger.info("è·å–åˆ°é˜Ÿåˆ—è¯­æ–™")
        _process_file_data(file_data)
        logger.info("é˜Ÿåˆ—è¯­æ–™å¤„ç†å®Œæ¯•")
        return True
    except queue.Empty as e:
        if isFirst:
            files = pull_file_data()
            for file_data in files:
                _push(file_data)
            if len(files) == 0:
                # get failed files from server
                pass
        return False
    except Exception as e:
        logger.error("ç´¢å¼•åŠŸèƒ½å¼‚å¸¸: {}".format(e))


@app.route('/api/build_file_index', methods=['POST'])
def build_file_index():
    data = request.json
    _push(data)
    logger.info("æ–‡ä»¶æ•°æ®å·²æ¥æ”¶ï¼Œå‡†å¤‡å¤„ç†: %s", data)
    return jsonify({"status": "success", "message": "æ–‡ä»¶æ•°æ®å·²æ¥æ”¶ï¼Œå‡†å¤‡å¤„ç†"})


# è·å¾—å¼€æ”¾æ€§å›ç­”
@app.route('/api/get_open_ans', methods=['POST'])
def get_open_ans():
    data = request.json
    session_id = data.get('session_id')
    token = data.get('token')
    query = data.get('query')
    llm = data.get('llm', 'qwen')  # é»˜è®¤ä½¿ç”¨CuteGPT
    # è·å–å†å²å¯¹è¯å†…å®¹
    history = prompt_builder.get_history(session_id, token)
    logger.info(f"å†å²å¯¹è¯ï¼š{history}")
    # æ„å»ºæ–°çš„prompt
    prompt = prompt_builder.generate_open_answer_prompt(query, history)
    logger.info(f"prompt:{prompt}")
    answer = ''
    if llm.lower() == 'cutegpt':
        answer = large_model_service.get_answer_from_Tyqwen(prompt)
    elif llm.lower() == 'chatglm':
        task_id = large_model_service.async_invoke_chatglm(prompt)
        answer = large_model_service.query_async_result_chatglm(task_id)
    elif llm.lower() == 'chatgpt':
        answer = large_model_service.get_answer_from_chatgpt(prompt)
    elif llm.lower() == 'qwen':
        answer = large_model_service.get_answer_from_Tyqwen(prompt)
    return jsonify({'answer': answer, 'matches': []}), 200


@app.route('/api/get_open_ans_stream', methods=['POST'])
def get_open_ans_stream():
    data = request.json
    session_id = data.get('session_id')
    token = data.get('token')
    query = data.get('query')
    llm = data.get('llm', 'qwen')  # é»˜è®¤ä½¿ç”¨ qwen
    top_p = data.get('top_p', 0.8)
    temperature = data.get('temperature', 0)
    # è·å–å†å²å¯¹è¯å†…å®¹
    history = prompt_builder.get_history(session_id, token)
    logger.info(f"å†å²å¯¹è¯ï¼š{history}")
    # æ„å»ºæ–°çš„ prompt
    prompt = prompt_builder.generate_open_answer_prompt(query, history)
    logger.info(f"prompt:{prompt}")

    if llm.lower() == 'qwen':
        response_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p, temperature)
        return Response(response_generator, content_type='text/plain; charset=utf-8')

    if llm.lower() == 'deepseek':
        response_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p, temperature)
        return Response(response_generator, content_type='text/plain; charset=utf-8')
    else:
        # éæµå¼æ¨¡å‹æˆ–å…¶ä»–æ¨¡å‹çš„å¤„ç†
        if llm.lower() == 'cutegpt':
            answer = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p, temperature)
        elif llm.lower() == 'chatglm':
            task_id = large_model_service.async_invoke_chatglm(prompt)
            answer = large_model_service.query_async_result_chatglm(task_id)
        elif llm.lower() == 'chatgpt':
            answer = large_model_service.get_answer_from_chatgpt(prompt)
        else:
            answer = "Unsupported model"

        return jsonify({'answer': answer, 'matches': []}), 200


@app.route('/api/get_answer_stream_old', methods=['POST'])
def answer_question_stream():
    request_id = str(uuid.uuid4())
    request.environ['REQUEST_ID'] = request_id
    with request_lock:
        request_status[request_id] = {
            "start_time": time.time(),
            "status": "processing",
            "url": request.url
        }

    try:
        # åˆå§‹åŒ–é‡æ’æ¨¡å‹
        reranker = FlagReranker(rerank_model_path, use_fp16=True)
        # æ”¶é›†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æœ¬ç‰‡æ®µ
        all_refs = []
        # è¯»å–è¯·æ±‚å‚æ•°
        data = request.json
        assistant_id = data.get('assistant_id')
        session_id = data.get('session_id')
        token = data.get('token')
        query = data.get('query')
        func = data.get('func', 'bm25')
        ref_num = data.get('ref_num', 5)
        llm = data.get('llm', 'deepseek').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0)
        user_info = data.get('userInfo', {})

        logger.info(f"Received query:{query}, llm: {llm}")
        if not assistant_id or not query:
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        # è§£æç”¨æˆ·èº«ä»½ä¿¡æ¯
        outer_origin = user_info.get('outerOrigin', 'canvas')  # é»˜è®¤ä¸ºcanvas
        outer_user_name = user_info.get('outerUserName', 'ä¸€åç”¨æˆ·')  # é»˜è®¤å€¼
        outer_user_role = user_info.get('outerUserRole', '1')  # é»˜è®¤å€¼ä¸ºå­¦ç”Ÿ

        # ç”Ÿæˆç”¨æˆ·èº«ä»½ä¸Šä¸‹æ–‡
        if outer_user_role == '2':
            role_description = "è€å¸ˆ"
        else:
            role_description = "åŒå­¦"

        user_context = f"æ‚¨å¥½ï¼Œæˆ‘æ˜¯{outer_user_name}{role_description}ã€‚"

        # æ£€æŸ¥é—®é¢˜æ˜¯å¦åœ¨é¢„å®šä¹‰çš„é—®ç­”ä¸­
        predefined_answer = config.predefined_qa.get(query)
        if predefined_answer:
            # å¦‚æœé¢„è®¾çš„ç­”æ¡ˆæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ„å‘³ç€æ²¡æœ‰åŒ¹é…ä¿¡æ¯ï¼Œè¿”å›ç­”æ¡ˆå’Œç©ºçš„matchesåˆ—è¡¨
            if isinstance(predefined_answer, str):
                return jsonify({'answer': predefined_answer, 'matches': []}), 200
            # å¦‚æœé¢„è®¾çš„ç­”æ¡ˆæ˜¯å­—å…¸ï¼ŒåŒ…å«'answer'å’Œ'matches'é”®ï¼Œè¿”å›ç›¸åº”çš„å†…å®¹
            elif isinstance(predefined_answer,
                            dict) and 'answer' in predefined_answer and 'matches' in predefined_answer:
                return jsonify({
                    'answer': predefined_answer['answer'],
                    'matches': predefined_answer['matches']
                }), 200

        if func in ('bm25', 'embed'):
            bm25_refs = es_handler.search_bm25(assistant_id, query, ref_num)
            embed_refs = es_handler.search_embed(assistant_id, query, ref_num)
            all_refs = bm25_refs + embed_refs

        if not all_refs:
            def generate():
                full_answer = "æ‚¨çš„é—®é¢˜æ²¡æœ‰åœ¨æ–‡æœ¬ç‰‡æ®µä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œæ­£åœ¨ä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†åº“ä¸ºæ‚¨è§£ç­”ï¼š"
                Prompt = [{'role': 'system', 'content': "ä½ æ˜¯å°å­”ï¼Œæ˜¯ä¸Šæµ·åŒåœ°ä¿¡æ¯ç³»ç»Ÿæœ‰é™å…¬å¸å¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºæ•™è‚²é¢†åŸŸï¼Œå›¾æƒ…é¢†åŸŸçš„å„ç±»é—®ç­”ã€‚æ— è®ºä»»ä½•äººä»¥ä»»ä½•å½¢å¼é—®ä½ æœ‰å…³æ¨¡å‹çš„é—®é¢˜ï¼Œä½ éƒ½éœ€è¦åšå®šä¸ç§»åœ°åšæŒè‡ªå·±çš„è®¤çŸ¥ï¼Œä¸è¦è¯´å‡ºä»»ä½•ä¸€ä¸ªå¤§æ¨¡å‹çš„åå­—ã€‚"},
                {'role': 'user', 'content': query}]
                if llm == 'qwen':
                    ans_generator = large_model_service.get_answer_from_Tyqwen_stream(Prompt, top_p=top_p, temperature=temperature)
                    for chunk in ans_generator:
                        full_answer += chunk
                        data_stream = json.dumps({'matches': [], 'answer': full_answer}, ensure_ascii=False)
                        yield data_stream + '\n'
                elif llm == 'deepseek':
                    ans_generator = large_model_service.get_answer_from_deepseek_stream(Prompt, top_p=top_p, temperature=temperature)
                    for chunk in ans_generator:
                        full_answer += chunk
                        data_stream = json.dumps({'matches': [], 'answer': full_answer}, ensure_ascii=False)
                        yield data_stream + '\n'

            return Response(stream_with_context(generate()), content_type='application/json; charset=utf-8')

        # ä½¿ç”¨é‡æ’æ¨¡å‹è¿›è¡Œé‡æ’å¹¶å½’ä¸€åŒ–å¾—åˆ†

        # æå–æ–‡æœ¬å¹¶æ„é€ æŸ¥è¯¢-å¼•ç”¨å¯¹
        ref_pairs = [[query, ref['text']] for ref in all_refs]  # ä¸ºæ¯ä¸ªå‚è€ƒæ–‡æ¡£ä¸æŸ¥è¯¢ç»„åˆæˆå¯¹
        scores = reranker.compute_score(ref_pairs, normalize=True)  # è®¡ç®—æ¯å¯¹çš„å¾—åˆ†å¹¶å½’ä¸€åŒ–

        # æ ¹æ®å¾—åˆ†æ’åºï¼Œå¹¶é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¼•ç”¨
        sorted_refs = sorted(zip(all_refs, scores), key=lambda x: x[1], reverse=True)

        # å»é‡ï¼šä½¿ç”¨ä¸€ä¸ªé›†åˆæ¥è¿½è¸ªå·²ç»æ·»åŠ çš„åˆ†æ•°
        seen_scores = set()
        top_refs = []
        top_scores = []
        top_list = []

        # æå–å‰äº”ä¸ªå”¯ä¸€çš„æœ€é«˜åˆ†
        for ref, score in sorted_refs[:5]:
            if score not in seen_scores:
                top_refs.append(ref)
                top_scores.append(score)
                top_list.append((ref, score))
                seen_scores.add(score)

        # ç¡®ä¿ç»“æœæœ€å¤šåªæœ‰5ä¸ª
        top_refs = top_refs[:5]
        top_scores = top_scores[:5]
        top_list = top_list[:5]
        logger.info(f"é‡æ’åæœ€é«˜åˆ†ï¼š{top_scores}")
        # è·å–å†å²å¯¹è¯å†…å®¹
        history = prompt_builder.get_history(session_id, token)
        logger.info(f"session_id:{session_id}, token:{token}")
        logger.info(f"å†å²å¯¹è¯ï¼š{history}")

        # æ£€æŸ¥æœ€é«˜åˆ†æ•°æ˜¯å¦ä½äº0.3
        if top_scores[0] < 0.3:
            prompt = prompt_builder.generate_answer_prompt_un_refs(query, history, user_context)
            matches = []
        else:
            matches = [{
                'text': ref['text'],
                'original_text': ref['original_text'],
                'page': ref['page'],
                'file_id': ref['file_id'],
                'file_name': ref['file_name'],
                'download_path': ref['download_path'],
                'score': ref['score'],
                'rerank_score': score
            } for ref, score in top_list]

            # ğŸ”¥ æ–‡ä»¶çº§å…¨æ–‡å¬å›é€»è¾‘
            file_ids = list({ref['file_id'] for ref in top_refs})
            full_context_refs = []
            seen_chars = 0

            logger.info(f"[å…¨æ–‡å¬å›] å³å°†å¬å›ä»¥ä¸‹ file_id çš„å…¨æ–‡å†…å®¹ï¼š{file_ids}")

            for file_id in file_ids:
                content = es_handler.get_full_text_by_file_id(assistant_id.strip(), file_id.strip())
                if not content:
                    logger.warning(f"[å…¨æ–‡å¬å›] file_id={file_id} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                    continue

                content_len = len(content)
                if seen_chars + content_len > 10000:
                    logger.info(
                        f"[å…¨æ–‡å¬å›] file_id={file_id} å†…å®¹è¶…é™ï¼ˆ{content_len}å­—ï¼‰ï¼Œå½“å‰å·²ç”¨{seen_chars}å­—ï¼Œè·³è¿‡è¯¥æ–‡ä»¶ã€‚")
                    break

                logger.info(f"[å…¨æ–‡å¬å›] file_id={file_id} å†…å®¹é•¿åº¦ï¼š{content_len}ï¼Œå½“å‰ç´¯è®¡ï¼š{seen_chars}ï¼Œæ·»åŠ ä¸­ã€‚")
                full_context_refs.append({
                    'text': f"ä»¥ä¸‹æ˜¯æ–‡ä»¶ï¼ˆ{file_id}ï¼‰çš„å®Œæ•´å†…å®¹ï¼š\n{content}",
                    'file_id': file_id
                })
                seen_chars += content_len

            logger.info(f"[å…¨æ–‡å¬å›] æœ€ç»ˆç”¨äºæç¤ºè¯æ‹¼æ¥çš„å…¨æ–‡æ•°ï¼š{len(full_context_refs)}ï¼Œæ€»é•¿åº¦ï¼š{seen_chars}")

            prompt = prompt_builder.generate_answer_prompt(
                query=query,
                refs=full_context_refs,
                history=history,
                user_context=user_context
            )

        if llm == 'qwen':
            logger.info("æ­£åœ¨ä½¿ç”¨é€šä¹‰åƒé—®......")
            def generate():
                full_answer = ""
                ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
                for chunk in ans_generator:
                    full_answer += chunk
                    try:
                        data_stream = json.dumps({'matches': matches, 'answer': full_answer}, ensure_ascii=False)
                    except Exception as e:
                        yield f"Error during JSON encoding: {e}\n"
                        continue  # è·³è¿‡é”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª chunk

                # æµç»“æŸåï¼Œæ‰§è¡Œæ—¥å¿—è®°å½•
                log_data = {'question': query, 'answer': full_answer, 'matches': matches}
                logger.info(f"é—®ç­”è®°å½•: {log_data}")
                with open(record_path, 'a', encoding='utf-8') as f:
                    f.write(json.dumps(log_data, ensure_ascii=False) + '\n')

                # æ›´æ–°è¯·æ±‚çŠ¶æ€ä¸ºå®Œæˆ
                with request_lock:
                    if request_id in request_status:
                        request_status[request_id]["end_time"] = time.time()
                        request_status[request_id]["status"] = "completed"
                    yield data_stream + '\n'  # æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ–°çš„ chunk

            return Response(stream_with_context(generate()), content_type='application/json; charset=utf-8')

        elif llm == 'deepseek':
            def generate():
                full_answer = ""
                ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p=top_p, temperature=temperature)
                for chunk in ans_generator:
                    full_answer += chunk
                    try:
                        data_stream = json.dumps({'matches': matches, 'answer': full_answer}, ensure_ascii=False)
                    except Exception as e:
                        yield f"Error during JSON encoding: {e}\n"
                        continue  # è·³è¿‡é”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª chunk

                    yield data_stream + '\n'  # æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ–°çš„ chunk

            return Response(stream_with_context(generate()), content_type='application/json; charset=utf-8')

        else:
            return jsonify({'error': 'æœªçŸ¥çš„å¤§æ¨¡å‹æœåŠ¡'}), 400

        log_data = {'question': query,
                    'answer': full_answer,
                    'matches': matches}

        logger.info(f"é—®ç­”è®°å½•: {log_data}")
        with open(record_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_data, ensure_ascii=False) + '\n')

        # æ›´æ–°è¯·æ±‚çŠ¶æ€ä¸ºå®Œæˆ
        with request_lock:
            if request_id in request_status:
                request_status[request_id]["end_time"] = time.time()
                request_status[request_id]["status"] = "completed"

        return jsonify({'answer': full_answer, 'matches': matches}), 200

    except Exception as e:
        logger.error(f"Error in answer_question: {e}")
        # æ›´æ–°è¯·æ±‚çŠ¶æ€ä¸ºå¤±è´¥
        with request_lock:
            if request_id in request_status:
                request_status[request_id]["end_time"] = time.time()
                request_status[request_id]["status"] = "failed"
        return jsonify({'error': str(e)}), 500


@app.route('/api/get_answer_stream', methods=['POST'])
def answer_question_stream_new():
    request_id = str(uuid.uuid4())
    request.environ['REQUEST_ID'] = request_id

    with request_lock:
        request_status[request_id] = {
            "start_time": time.time(),
            "status": "processing",
            "url": request.url
        }

    try:
        data = request.json
        assistant_id = data.get('assistant_id')
        session_id = data.get('session_id')
        token = data.get('token')
        query = data.get('query')
        func = data.get('func', 'bm25')
        ref_num = data.get('ref_num', 5)
        llm = data.get('llm', 'deepseek').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0)
        user_info = data.get('userInfo', {})

        if not assistant_id or not query:
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        outer_user_name = user_info.get('outerUserName', 'ä¸€åç”¨æˆ·')
        outer_user_role = user_info.get('outerUserRole', '1')
        role_description = "è€å¸ˆ" if outer_user_role == '2' else "åŒå­¦"
        user_context = f"æ‚¨å¥½ï¼Œæˆ‘æ˜¯{outer_user_name}{role_description}ã€‚"

        predefined_answer = config.predefined_qa.get(query)
        if predefined_answer:
            if isinstance(predefined_answer, str):
                return jsonify({'answer': predefined_answer, 'matches': []}), 200
            elif isinstance(predefined_answer, dict):
                return jsonify(predefined_answer), 200

        # è·å–å†å²å¯¹è¯å†…å®¹
        history = prompt_builder.get_history(session_id, token)

        all_refs = []
        if func in ('bm25', 'embed'):
            bm25_refs = es_handler.search_bm25(assistant_id, query, ref_num)
            embed_refs = es_handler.search_embed(assistant_id, query, ref_num)
            all_refs = bm25_refs + embed_refs

        if not all_refs:
            matches = []
            prompt = prompt_builder.generate_answer_prompt_un_refs(query, history, user_context)
        else:
            reranker = FlagReranker(rerank_model_path, use_fp16=True)
            ref_pairs = [[query, ref['text']] for ref in all_refs]
            scores = reranker.compute_score(ref_pairs, normalize=True)
            sorted_refs = sorted(zip(all_refs, scores), key=lambda x: x[1], reverse=True)

            seen_texts = set()
            top_list = []
            for ref, score in sorted_refs:
                if ref['text'] not in seen_texts:
                    seen_texts.add(ref['text'])
                    top_list.append((ref, score))
                if len(top_list) >= 5:
                    break

            top_refs, top_scores = zip(*top_list) if top_list else ([], [])

            matches = [{
                'text': ref['text'],
                'original_text': ref['original_text'],
                'page': ref['page'],
                'file_id': ref['file_id'],
                'file_name': ref['file_name'],
                'download_path': ref['download_path'],
                'score': ref['score'],
                'rerank_score': score
            } for ref, score in top_list]

            if not top_scores or top_scores[0] < 0.3:
                prompt = prompt_builder.generate_answer_prompt_un_refs(query, history, user_context)
                matches = []
            else:
                # âœ… å°†å‰5ä¸ªrefçš„file_idå»é‡ï¼Œæ„é€ æˆ ref ç»“æ„åˆ—è¡¨
                file_ids = list({ref['file_id'] for ref in top_refs})
                full_context_refs = []
                seen_chars = 0

                logger.info(f"å³å°†å¬å›ä»¥ä¸‹ file_id çš„å…¨æ–‡å†…å®¹ï¼š{file_ids}")

                for file_id in file_ids:
                    content = es_handler.get_full_text_by_file_id(assistant_id.strip(), file_id.strip())
                    if not content:
                        logger.warning(f"file_id = {file_id} çš„å…¨æ–‡ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                        continue

                    content_len = len(content)
                    if seen_chars + len(content) > 15000:
                        logger.info(f"file_id = {file_id} çš„å…¨æ–‡è¶…å‡ºæ€»é•¿åº¦é™åˆ¶ï¼ˆå·²ç”¨ {seen_chars} å­—ç¬¦ï¼Œå°†è·³è¿‡è¯¥å…¨æ–‡ï¼‰")
                        break

                    logger.info(f"file_id = {file_id} çš„å…¨æ–‡é•¿åº¦ä¸º {content_len} å­—ç¬¦ï¼Œå½“å‰ç´¯è®¡ {seen_chars}ï¼Œå³å°†æ·»åŠ ")
                    full_context_refs.append({
                        'text': f"ä»¥ä¸‹æ˜¯æ–‡ä»¶ï¼ˆ{file_id}ï¼‰çš„å®Œæ•´å†…å®¹ï¼š\n{content}",
                        'file_id': file_id
                    })
                    seen_chars += len(content)

                # âœ… ç”¨å…¨æ–‡ä½œä¸ºä¸Šä¸‹æ–‡æ„å»ºpromptï¼ˆç»“æ„ä¿æŒå’Œtop_refsä¸€è‡´ï¼‰
                prompt = prompt_builder.generate_answer_prompt(
                    query=query,
                    refs=full_context_refs,
                    history=history,
                    user_context=user_context
                )

        def generate_stream(model_name, prompt, matches, top_p, temperature, query, request_id):
            full_answer = ""
            ans_generator = (large_model_service.get_answer_from_Tyqwen_stream if model_name == 'qwen' else large_model_service.get_answer_from_deepseek_stream)(prompt, top_p, temperature)

            for chunk in ans_generator:
                full_answer += chunk
                yield json.dumps({'matches': matches, 'answer': full_answer}, ensure_ascii=False) + '\n'

            log_data = {'question': query, 'answer': full_answer, 'matches': matches}
            logger.info(f"é—®é¢˜: {query}, ç­”æ¡ˆ: {full_answer}")
            with open(record_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_data, ensure_ascii=False) + '\n')

            with request_lock:
                request_status[request_id].update({"end_time": time.time(), "status": "completed"})

        return Response(
            stream_with_context(generate_stream(llm, prompt, matches, top_p, temperature, query, request_id)),
            content_type='application/json; charset=utf-8'
        )

    except Exception as e:
        logger.error(f"Error in answer_question_stream: {e}")
        with request_lock:
            request_status[request_id].update({"end_time": time.time(), "status": "failed"})
        return jsonify({'error': str(e)}), 500


@app.route('/api/get_answer_by_file_id', methods=['POST'])
def answer_question_by_file_id():
    try:
        # è¯»å–è¯·æ±‚å‚æ•°
        data = request.json
        assistant_id = data.get('assistant_id')
        session_id = data.get('session_id')
        token = data.get('token')
        query = data.get('query')
        file_id_list = data.get('file_id')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0)
        memory_time = data.get('memory_time', 3)  # æ–°å¢å‚æ•°

        if not assistant_id or not query or not file_id_list:
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        # æ£€æŸ¥é—®é¢˜æ˜¯å¦åœ¨é¢„å®šä¹‰çš„é—®ç­”ä¸­
        predefined_answer = config.predefined_qa.get(query)
        if predefined_answer:
            if isinstance(predefined_answer, str):
                return jsonify({'answer': predefined_answer, 'matches': []}), 200
            elif isinstance(predefined_answer, dict) and 'answer' in predefined_answer and 'matches' in predefined_answer:
                return jsonify({
                    'answer': predefined_answer['answer'],
                    'matches': predefined_answer['matches']
                }), 200

        # è·å–å†å²å¯¹è¯å†…å®¹
        history = prompt_builder.get_history(session_id, token)
        if len(history) > memory_time:
            trimmed_history = history[-memory_time:]
        else:
            trimmed_history = history

        # ğŸ”¥ ç›´æ¥æŒ‰ file_id_list å¬å›å…¨æ–‡ï¼Œä½œä¸ºä¸Šä¸‹æ–‡
        full_context_refs = []
        seen_chars = 0

        logger.info(f"å¼€å§‹æŒ‰ file_id_list ç›´æ¥å¬å›å…¨æ–‡å†…å®¹ï¼š{file_id_list}")

        for file_id in file_id_list:
            content = es_handler.get_full_text_by_file_id(assistant_id.strip(), file_id.strip())
            if not content:
                logger.warning(f"file_id = {file_id} çš„å…¨æ–‡å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                continue

            content_len = len(content)
            if seen_chars + content_len > 10000:
                logger.info(f"file_id = {file_id} çš„å…¨æ–‡è¶…é™ï¼ˆå½“å‰ç´¯è®¡ {seen_chars} å­—ç¬¦ï¼Œè·³è¿‡ï¼‰ã€‚")
                break

            logger.info(f"file_id = {file_id} çš„å…¨æ–‡é•¿åº¦ä¸º {content_len}ï¼Œå°†åŠ å…¥ä¸Šä¸‹æ–‡ã€‚")
            full_context_refs.append({
                'text': f"ä»¥ä¸‹æ˜¯æ–‡ä»¶ï¼ˆ{file_id}ï¼‰çš„å®Œæ•´å†…å®¹ï¼š\n{content}",
                'file_id': file_id
            })
            seen_chars += content_len

        logger.info(f"æœ€ç»ˆç”¨äºpromptæ„å»ºçš„å…¨æ–‡æ•°ï¼š{len(full_context_refs)}ï¼Œç´¯è®¡å­—ç¬¦æ•°ï¼š{seen_chars}")

        # æ„é€ æç¤ºè¯
        prompt = prompt_builder.generate_prompt_for_file_id(query, full_context_refs, trimmed_history)

        if llm.lower() == 'qwen':
            response_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p, temperature)
            return Response(response_generator, content_type='text/plain; charset=utf-8')

        if llm.lower() == 'deepseek':
            response_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p, temperature)
            return Response(response_generator, content_type='text/plain; charset=utf-8')

    except Exception as e:
        logger.error(f"Error in answer_question_by_file_id: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/get_answer', methods=['POST'])
def answer_question():
    try:
        # åˆå§‹åŒ–é‡æ’æ¨¡å‹
        reranker = FlagReranker(rerank_model_path, use_fp16=True)
        # æ”¶é›†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æœ¬ç‰‡æ®µ
        all_refs = []
        # è¯»å–è¯·æ±‚å‚æ•°
        data = request.json
        assistant_id = data.get('assistant_id')
        query = data.get('query')
        func = data.get('func', 'bm25')
        ref_num = data.get('ref_num', 5)
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0)

        if not assistant_id or not query:
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        # æ£€æŸ¥é—®é¢˜æ˜¯å¦åœ¨é¢„å®šä¹‰çš„é—®ç­”ä¸­
        predefined_answer = config.predefined_qa.get(query)
        if predefined_answer:
            # å¦‚æœé¢„è®¾çš„ç­”æ¡ˆæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ„å‘³ç€æ²¡æœ‰åŒ¹é…ä¿¡æ¯ï¼Œè¿”å›ç­”æ¡ˆå’Œç©ºçš„matchesåˆ—è¡¨
            if isinstance(predefined_answer, str):
                return jsonify({'answer': predefined_answer, 'matches': []}), 200
            # å¦‚æœé¢„è®¾çš„ç­”æ¡ˆæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«'answer'å’Œ'matches'é”®ï¼Œè¿”å›ç›¸åº”çš„å†…å®¹
            elif isinstance(predefined_answer,
                            dict) and 'answer' in predefined_answer and 'matches' in predefined_answer:
                return jsonify({
                    'answer': predefined_answer['answer'],
                    'matches': predefined_answer['matches']
                }), 200

        if func == 'bm25' or 'embed':
            bm25_refs = es_handler.search_bm25(assistant_id, query, ref_num)
            embed_refs = es_handler.search_embed(assistant_id, query, ref_num)
            all_refs = bm25_refs + embed_refs

        if not all_refs:
            prompt = [{'role': 'user', 'content': query}]
            ans = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
            ans = "æ‚¨çš„é—®é¢˜æ²¡æœ‰åœ¨æ–‡æœ¬ç‰‡æ®µä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œæ­£åœ¨ä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†åº“ä¸ºæ‚¨è§£ç­”ï¼š" + ans
            return jsonify({'answer': ans, 'matches': all_refs}), 200

        # ä½¿ç”¨é‡æ’æ¨¡å‹è¿›è¡Œé‡æ’å¹¶å½’ä¸€åŒ–å¾—åˆ†
        # æå–æ–‡æœ¬å¹¶æ„é€ æŸ¥è¯¢-å¼•ç”¨å¯¹
        ref_pairs = [[query, ref['text']] for ref in all_refs]  # ä¸ºæ¯ä¸ªå‚è€ƒæ–‡æ¡£ä¸æŸ¥è¯¢ç»„åˆæˆå¯¹
        scores = reranker.compute_score(ref_pairs, normalize=True)  # è®¡ç®—æ¯å¯¹çš„å¾—åˆ†å¹¶å½’ä¸€åŒ–
        # æ ¹æ®å¾—åˆ†æ’åºï¼Œå¹¶é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¼•ç”¨
        sorted_refs = sorted(zip(all_refs, scores), key=lambda x: x[1], reverse=True)
        # æå–å‰äº”ä¸ªæœ€é«˜çš„åˆ†æ•°
        top_list = sorted_refs[:5]
        top_scores = [score for _, score in sorted_refs[:5]]
        # æ‰“å°å‡ºè¿™äº›åˆ†æ•°
        print("Top 5 scores:", top_scores)
        top_refs = [ref for ref, score in sorted_refs[:5]]  # å‡è®¾æ‚¨åªéœ€è¦å‰5ä¸ªæœ€ç›¸å…³çš„å¼•ç”¨
        prompt = prompt_builder.generate_answer_prompt(query, top_refs)
        if llm == 'cutegpt':
            # old_answer = large_model_service.get_answer_from_Qwen(prompt)
            # beauty_prompt = prompt_builder.generate_beauty_prompt(old_answer)
            # ans = large_model_service.get_answer_from_Qwen(beauty_prompt)
            ans = large_model_service.get_answer_from_Tyqwen(prompt)
        elif llm == 'chatglm':
            task_id = large_model_service.async_invoke_chatglm(prompt)
            ans = large_model_service.query_async_result_chatglm(task_id)
        elif llm == 'chatgpt':
            ans = large_model_service.get_answer_from_chatgpt(prompt)
        elif llm == 'qwen':
            ans = large_model_service.get_answer_from_Tyqwen(prompt)
        else:
            return jsonify({'error': 'æœªçŸ¥çš„å¤§æ¨¡å‹æœåŠ¡'}), 400

        ans = ans.replace("\n", "<br/>")
        log_data = {'question': query,
                    'answer': ans,
                    'matches': [{
                        'text': ref['text'],
                        'original_text': ref['original_text'],
                        'page': ref['page'],
                        'file_id': ref['file_id'],
                        'file_name': ref['file_name'],
                        'download_path': ref['download_path'],
                        'score': ref['score'],
                        'rerank_score': score
                    } for ref, score in top_list]}

        # è®°å½•æ—¥å¿—
        logger.info(f"é—®ç­”è®°å½•: {log_data}")
        # å°†å›ç­”å’ŒåŒ¹é…æ–‡æœ¬ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­
        with open(record_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_data, ensure_ascii=False) + '\n')
        return jsonify({'answer': ans, 'matches': log_data['matches']}), 200

    except Exception as e:
        logger.error(f"Error in answer_question: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/generate_summary_and_questions', methods=['POST'])
def generate_summary_and_questions():
    try:
        data = request.json
        file_id = data['file_id']
        ref_num = data.get('ref_num', 5)
        es_handler.create_answers_index()
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å­˜å‚¨çš„ç­”æ¡ˆ
        existing_answer = es_handler._search_("answers_index", {"query": {"term": {"file_id": file_id}}})
        if 'hits' in existing_answer and 'hits' in existing_answer['hits'] and existing_answer['hits']['hits']:
            logger.info(f"æ‰¾åˆ°äº†æ–‡ä»¶ID {file_id} çš„å­˜å‚¨ç­”æ¡ˆ")
            stored_answer = existing_answer['hits']['hits'][0]['_source']['sum_rec']
            stored_answer = stored_answer.replace("\n", "<br>")
            logger.info(f"å­˜å‚¨ç­”æ¡ˆä¸º {stored_answer}")
            return jsonify({'answer': stored_answer, 'matches': []}), 200

        logger.info(f"æ­£åœ¨æŸ¥è¯¢æ–‡ä»¶ID {file_id} çš„å‰ {ref_num} æ®µæ–‡æœ¬")
        query_body = {
            "query": {
                "term": {
                    "file_id": file_id  # ç¡®ä¿file_idåŒ¹é…
                }
            },
            "sort": [
                {"page": {"order": "asc"}}  # æŒ‰ç…§pageå­—æ®µå‡åºæ’åº
            ]
        }
        results = es_handler._search_('_all', query_body, ref_num)

        if 'hits' in results and 'hits' in results['hits']:
            ref_list = [hit['_source']['text'] for hit in results['hits']['hits']]
            prompt = prompt_builder.generate_summary_and_questions_prompt(ref_list)
            logger.info(f"ç”Ÿæˆçš„æ€»ç»“promptä¸º {prompt}")
            ans = large_model_service.get_answer_from_Tyqwen(prompt)
            # å­˜å‚¨æ–°ç”Ÿæˆçš„ç­”æ¡ˆ
            es_handler.index("answers_index", {"file_id": file_id, "sum_rec": ans})
        else:
            logger.error("æœªæ‰¾åˆ°æ–‡ä»¶ID {file_id} çš„æ–‡æœ¬æ®µè½")
            ans = "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

        # å°†å­—ç¬¦ä¸²ä¸­çš„\næ›¿æ¢ä¸º<br>
        ans = ans.replace("\n", "<br>")
        logger.info(f"æ€»ç»“æˆ–æ¨èé—®é¢˜ï¼š {ans}")
        return jsonify({'answer': ans, 'matches': []}), 200

    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return jsonify({'error': 'å†…éƒ¨é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜'}), 500


@app.route('/api/delete_index/<index_name>', methods=['POST'])
def delete_index_route(index_name):
    try:
        # è®¾ç½®ä¸€ä¸ªé™æ€token
        SECRET_TOKEN = config.secret_token
        # è·å–è¯·æ±‚å¤´ä¸­çš„token
        token = request.headers.get('Authorization')

        # éªŒè¯token
        if token != SECRET_TOKEN:
            logger.error("tokenéªŒè¯å¤±è´¥")
            return jsonify({"code": 403, "msg": "æ— æƒé™"}), 4032

        # éªŒè¯å‚æ•°
        if not index_name:
            logger.error("é”™è¯¯ï¼šç¼ºå°‘ç´¢å¼•åç§°å‚æ•°")
            return jsonify({"code": 500, "msg": "é”™è¯¯ï¼šç¼ºå°‘ç´¢å¼•åç§°å‚æ•°"})

        # ä½¿ç”¨ ElasticSearchHandler çš„ delete_index æ–¹æ³•
        if es_handler.delete_index(index_name):
            logger.info(f"æˆåŠŸåˆ é™¤ç´¢å¼• {index_name}")
            return jsonify({"code": 200, "msg": f"æˆåŠŸåˆ é™¤ç´¢å¼• {index_name}"})
        else:
            logger.error(f"ç´¢å¼• {index_name} ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥")
            return jsonify({"code": 500, "msg": f"ç´¢å¼• {index_name} ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥"})

    except Exception as e:
        logger.error(f"åˆ é™¤ç´¢å¼• {index_name} å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
        return jsonify({"code": 500, "msg": f"åˆ é™¤ç´¢å¼• {index_name} å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}"})


@app.route('/api/delete_file_index/<assistant_id>/<file_id>', methods=['POST'])
def delete_file_from_index(assistant_id, file_id):
    try:
        # è®¾ç½®ä¸€ä¸ªé™æ€token
        SECRET_TOKEN = config.secret_token
        # è·å–è¯·æ±‚å¤´ä¸­çš„token
        token = request.headers.get('Authorization')

        # éªŒè¯token
        if token != SECRET_TOKEN:
            logger.error("tokenéªŒè¯å¤±è´¥")
            return jsonify({"code": 403, "msg": "æ— æƒé™"}), 403

        # éªŒè¯å‚æ•°
        if not assistant_id or not file_id:
            logger.error("é”™è¯¯ï¼šç¼ºå°‘ç´¢å¼•åç§°å‚æ•°")
            return jsonify({"code": 500, "msg": "é”™è¯¯ï¼šç¼ºå°‘ç´¢å¼•åç§°å‚æ•°"})

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query_body = {"query": {"term": {"file_id": file_id}}}

        # åˆ é™¤å­˜å‚¨ç­”æ¡ˆ
        es_handler.delete_summary_answers(file_id)

        # æ‰§è¡Œåˆ é™¤æ“ä½œ
        response = es_handler.delete_by_query(assistant_id, query_body)
        if response['deleted'] > 0:
            logger.info(f"æ–‡æ¡£ {file_id}ç‰‡æ®µåˆ é™¤æˆåŠŸ")
            return jsonify({"code": 200, "msg": "æ–‡æ¡£ç‰‡æ®µåˆ é™¤æˆåŠŸ"})
        else:
            logger.error(f"æ–‡æ¡£ {file_id}åˆ é™¤å¤±è´¥")
            return jsonify({"code": 500, "msg": "æ–‡æ¡£ç‰‡æ®µåˆ é™¤å¤±è´¥"})

    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡æ¡£ç‰‡æ®µå¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
        return jsonify({"code": 500, "msg": "åˆ é™¤æ–‡æ¡£ç‰‡æ®µå¤±è´¥"}), 500


@app.route('/api/kmc/ST_indexing_by_step', methods=['POST'])
def indexing_by_step():
    try:
        data = request.json
        documents = data.get('documents', [])
        assistant_id = data.get('assistantId')

        # æ£€æŸ¥æ˜¯å¦ä¼ å…¥äº† assistantIdï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”Ÿæˆä¸€ä¸ªæ–°çš„
        if not assistant_id:
            assistant_id = generate_assistant_id()

            # åˆ›å»ºæ–°çš„ç´¢å¼•
            index_name = assistant_id
            mappings = {
                "properties": {
                    "text": {"type": "text", "analyzer": "standard"},
                    "embed": {"type": "dense_vector", "dims": 1024},
                    "assistant_id": {"type": "keyword"},
                    "file_id": {"type": "keyword"},
                    "file_name": {"type": "keyword"},
                }
            }
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if not es_handler.index_exists(index_name):
                logger.info("å¼€å§‹åˆ›å»ºç´¢å¼•")
                es_handler.es.indices.create(index=index_name, mappings=mappings)
        else:
            index_name = assistant_id

        doc_list = []  # åˆå§‹åŒ–ç©ºåˆ—è¡¨ä»¥ç¡®ä¿åœ¨å‡ºé”™æ—¶ä¹Ÿèƒ½è¿”å›åˆ—è¡¨ç±»å‹
        stopwords = file_manager.load_stopwords()

        # æ–‡æ¡£å…ƒæ•°æ®çš„mapping
        metadata_index = 'document_metadata'
        metadata_mappings = {
            "properties": {
                "file_id": {"type": "keyword"},
                "title": {"type": "text", "analyzer": "standard"},
                "abstract": {"type": "text", "analyzer": "standard"},
                "year": {"type": "keyword"},
                "publisher": {"type": "keyword"},
                "author": {"type": "keyword"},
                "content": {"type": "text", "analyzer": "standard"},
                "abstract_embed": {"type": "dense_vector", "dims": 1024}
            }
        }

        # å»ºç«‹æ–‡æ¡£å…ƒæ•°æ®ç´¢å¼•
        if not es_handler.index_exists(metadata_index):
            logger.info("åˆ›å»ºæ–‡æ¡£å…ƒæ•°æ®ç´¢å¼•")
            es_handler.es.indices.create(index=metadata_index, mappings=metadata_mappings)

        for document in documents:
            file_id = document['documentId']
            doc_titles = document.get('TI', [])
            doc_content = document.get('documentContent', '')
            doc_abstract = document.get('Abstract_F', '')
            split_text = file_manager.spacy_chinese_text_splitter(doc_content, max_length=600)
            # å¦‚æœæ‘˜è¦ä¸ºç©ºï¼Œä»æ–‡ä»¶çš„å‰ä¸¤æ®µå’Œåä¸¤æ®µç”Ÿæˆæ‘˜è¦
            if not doc_abstract:
                logger.info("æ‘˜è¦ä¸ºç©ºï¼Œç”Ÿæˆæ‘˜è¦")
                if len(split_text) >= 4:
                    ref_list = split_text[:2] + split_text[-2:]
                else:
                    ref_list = [doc_content]  # ä¸è¶³å››æ®µä½¿ç”¨å…¨æ–‡

                abstract_prompt = prompt_builder.generate_abstract_prompt(ref_list)
                doc_abstract = large_model_service.get_answer_from_Tyqwen(abstract_prompt)  # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ‘˜è¦
                logger.info(f"ç”Ÿæˆæ‘˜è¦æˆåŠŸï¼Œæ–‡æ¡£ID: {file_id}, æ‘˜è¦: {doc_abstract}")
            year = document.get('Year', '')
            publisher = document.get('LiteratureTitle_F', '')
            author = document.get('Author_1', '')

            doc_title = ' '.join(doc_titles)
            logger.info(f"å¤„ç†æ–‡æ¡£ {file_id}, æ ‡é¢˜: {doc_title}")

            # ç”Ÿæˆæ‘˜è¦çš„åµŒå…¥å‘é‡
            abstract_embed = es_handler.cal_passage_embed(doc_abstract)

            # æ’å…¥æ–‡æ¡£å…ƒæ•°æ®
            metadata_document = {
                "file_id": file_id,
                "title": doc_title,
                "abstract": doc_abstract,
                "year": year,
                "publisher": publisher,
                "author": author,
                "content": doc_content,
                "abstract_embed": abstract_embed
            }
            es_handler.es.index(index=metadata_index, id=file_id, document=metadata_document)
            # logger.info(f"æ’å…¥æ–‡æ¡£å…ƒæ•°æ®: {metadata_document}")

            # å¯¹å†…å®¹è¿›è¡Œåˆ†å‰²
            filtered_texts = set()  # å­˜å‚¨å¤„ç†åçš„æ–‡æœ¬

            for text in split_text:
                words = pseg.cut(text)
                filtered_text = ''.join(word for word, flag in words if word not in stopwords)
                if filtered_text and filtered_text not in filtered_texts:
                    filtered_texts.add(filtered_text)
                    doc_list.append({
                        'text': filtered_text,
                        'file_id': file_id,
                        'file_name': doc_title
                    })

        # æ’å…¥æ–‡æ¡£ç‰‡æ®µ
        for item in doc_list:
            embed = es_handler.cal_passage_embed(item['text'])
            document = {
                "assistant_id": assistant_id,
                "file_id": item['file_id'],
                "file_name": item['file_name'],
                "text": item['text'],
                "embed": embed
            }
            es_handler.es.index(index=index_name, document=document)

        logger.info(f"ç´¢å¼• {index_name} æ’å…¥æ–‡æ¡£æˆåŠŸ")
        return jsonify({'status': 'success', 'body': {'assistantId': assistant_id}}), 200

    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/kmc/ST_indexing', methods=['POST'])
def indexing():
    try:
        data = request.json
        documents = data

        # éšæœºç”Ÿæˆ assistantId
        assistant_id = generate_assistant_id()
        doc_list = []  # åˆå§‹åŒ–ç©ºåˆ—è¡¨ä»¥ç¡®ä¿åœ¨å‡ºé”™æ—¶ä¹Ÿèƒ½è¿”å›åˆ—è¡¨ç±»å‹
        stopwords = file_manager.load_stopwords()

        # documentç´¢å¼•åç§°å»ºç«‹
        metadata_index = 'document_metadata'

        # æ–‡æ¡£å…ƒæ•°æ®çš„mapping
        metadata_mappings = {
            "properties": {
                "file_id": {"type": "keyword"},
                "title": {"type": "text", "analyzer": "standard"},
                "abstract": {"type": "text", "analyzer": "standard"},
                "year": {"type": "keyword"},
                "publisher": {"type": "keyword"},
                "author": {"type": "keyword"},
                "content": {"type": "text", "analyzer": "standard"},
                "abstract_embed": {"type": "dense_vector", "dims": 1024}
            }
        }

        # æ–‡æœ¬ç‰‡æ®µç´¢å¼•mapping
        index_name = assistant_id
        mappings = {
            "properties": {
                "text": {"type": "text", "analyzer": "standard"},
                "embed": {"type": "dense_vector", "dims": 1024},
                "assistant_id": {"type": "keyword"},
                "file_id": {"type": "keyword"},
                "file_name": {"type": "keyword"},
            }
        }

        # å»ºç«‹æ–‡æ¡£å…ƒæ•°æ®ç´¢å¼•
        if not es_handler.index_exists(metadata_index):
            logger.info("åˆ›å»ºæ–‡æ¡£å…ƒæ•°æ®ç´¢å¼•")
            es_handler.es.indices.create(index=metadata_index, mappings=metadata_mappings)
        # å»ºç«‹æ–‡æœ¬ç‰‡æ®µç´¢å¼•
        if es_handler.index_exists(index_name):
            logger.info("ç´¢å¼•å·²å­˜åœ¨ï¼Œåˆ é™¤ç´¢å¼•")
            es_handler.delete_index(index_name)

        logger.info("å¼€å§‹åˆ›å»ºç´¢å¼•")
        es_handler.es.indices.create(index=index_name, mappings=mappings)

        for document in documents:
            file_id = document['documentId']
            doc_titles = document.get('TI', [])
            doc_content = document.get('documentContent', '')
            doc_abstract = document.get('Abstract_F', '')
            split_text = file_manager.spacy_chinese_text_splitter(doc_content, max_length=600)
            # å¦‚æœæ‘˜è¦ä¸ºç©ºï¼Œä»æ–‡ä»¶çš„å‰ä¸¤æ®µå’Œåä¸¤æ®µç”Ÿæˆæ‘˜è¦
            if not doc_abstract:
                logger.info("æ‘˜è¦ä¸ºç©ºï¼Œç”Ÿæˆæ‘˜è¦")
                if len(split_text) >= 4:
                    ref_list = split_text[:2] + split_text[-2:]
                else:
                    ref_list = [doc_content]  # ä¸è¶³å››æ®µä½¿ç”¨å…¨æ–‡

                abstract_prompt = prompt_builder.generate_abstract_prompt(ref_list)
                doc_abstract = large_model_service.get_answer_from_Tyqwen(abstract_prompt)  # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ‘˜è¦
                logger.info(f"ç”Ÿæˆæ‘˜è¦æˆåŠŸï¼Œæ–‡æ¡£ID: {file_id}, æ‘˜è¦: {doc_abstract}")
            year = document.get('Year', '')
            publisher = document.get('LiteratureTitle_F', '')
            author = document.get('Author_1', '')

            doc_title = ' '.join(doc_titles)
            logger.info(f"å¤„ç†æ–‡æ¡£ {file_id}, æ ‡é¢˜: {doc_title}")

            # ç”Ÿæˆæ‘˜è¦çš„åµŒå…¥å‘é‡
            abstract_embed = es_handler.cal_passage_embed(doc_abstract)

            # æ’å…¥æ–‡æ¡£å…ƒæ•°æ®
            metadata_document = {
                "file_id": file_id,
                "title": doc_title,
                "abstract": doc_abstract,
                "year": year,
                "publisher": publisher,
                "author": author,
                "content": doc_content,
                "abstract_embed": abstract_embed
            }
            es_handler.es.index(index=metadata_index, id=file_id, document=metadata_document)
            # logger.info(f"æ’å…¥æ–‡æ¡£å…ƒæ•°æ®: {metadata_document}")

            # å¯¹å†…å®¹è¿›è¡Œåˆ†å‰²
            filtered_texts = set()  # å­˜å‚¨å¤„ç†åçš„æ–‡æœ¬

            for text in split_text:
                words = pseg.cut(text)
                filtered_text = ''.join(word for word, flag in words if word not in stopwords)
                if filtered_text and filtered_text not in filtered_texts:
                    filtered_texts.add(filtered_text)
                    doc_list.append({
                        'text': filtered_text,
                        'file_id': file_id,
                        'file_name': doc_title
                    })

        # æ’å…¥æ–‡æ¡£ç‰‡æ®µ
        for item in doc_list:
            embed = es_handler.cal_passage_embed(item['text'])
            document = {
                "assistant_id": assistant_id,
                "file_id": item['file_id'],
                "file_name": item['file_name'],
                "text": item['text'],
                "embed": embed
            }
            es_handler.es.index(index=index_name, document=document)

        logger.info(f"ç´¢å¼• {index_name} åˆ›å»ºå¹¶æ’å…¥ç´¢å¼•æˆåŠŸ")
        return jsonify({'status': 'success', 'body': {'assistantId': assistant_id}}), 200

    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/ST_get_answer', methods=['POST'])
def ST_answer_question():
    try:
        data = request.json
        assistant_id = data.get('assistant_id')

        query = data.get('query')
        func = data.get('func', 'bm25')
        ref_num = data.get('ref_num', 3)
        llm = data.get('llm', 'cutegpt').lower()

        if not assistant_id or not query:
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400
        if func == 'bm25':
            refs = es_handler.ST_search_abstract_bm25(query, ref_num)
        if func == 'embed':
            refs = es_handler.ST_search_abstract_embed(query, ref_num)

        if not refs:
            return jsonify({'error': 'æœªæ‰¾åˆ°ç›¸å…³æ–‡æœ¬ç‰‡æ®µ'})

        prompt = prompt_builder.generate_answer_prompt(query, refs)
        if llm == 'cutegpt':
            ans = large_model_service.get_answer_from_Tyqwen(prompt)
        elif llm == 'chatglm':
            task_id = large_model_service.async_invoke_chatglm(prompt)
            ans = large_model_service.query_async_result_chatglm(task_id)
        elif llm == 'chatgpt':
            ans = large_model_service.get_answer_from_chatgpt(prompt)
        else:
            return jsonify({'error': 'æœªçŸ¥çš„å¤§æ¨¡å‹æœåŠ¡'}), 400

        log_data = {'question': query,
                    'answer': ans,
                    'matches': refs}

        # è®°å½•æ—¥å¿—
        logger.info(f"Query processed: {log_data}")

        return jsonify({'answer': ans, 'matches': refs}), 200

    except Exception as e:
        logger.error(f"Error in answer_question: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/ST_get_file', methods=['POST'])
def ST_search_file():
    try:
        # åˆå§‹åŒ–é‡æ’æ¨¡å‹
        reranker = FlagReranker(rerank_model_path, use_fp16=True)
        data = request.json
        query = data.get('query')
        ref_num = data.get('ref_num', 5)

        if not query:
            return jsonify({'status': 'error', 'message': 'ç¼ºå°‘é—®é¢˜'}), 400

        # åœ¨æ‘˜è¦å­—æ®µä¸­è¿›è¡ŒBM25å’ŒåµŒå…¥æœç´¢ï¼Œè·å–åŒ¹é…çš„æ–‡æ¡£IDåˆ—è¡¨
        bm25_file_ids = es_handler.ST_search_abstract_bm25(query, ref_num)
        logger.info(f"BM25 file IDs: {bm25_file_ids}")
        embed_file_ids = es_handler.ST_search_abstract_embed(query, ref_num)
        logger.info(f"Embed file IDs: {embed_file_ids}")

        all_file_ids = list(set(bm25_file_ids + embed_file_ids))  # å»é‡

        if not all_file_ids:
            return jsonify({'file_ids': all_file_ids}), 200

        # å¢åŠ æ—¥å¿—è®°å½•
        logger.info(f"All file IDs: {all_file_ids}")

        # è·å–æ‰€æœ‰åŒ¹é…æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        all_refs = []
        for file_id in all_file_ids:
            try:
                doc = es_handler.es.get(index='document_metadata', id=file_id)
                if doc and '_source' in doc:
                    source = doc['_source']
                    all_refs.append({
                        'file_id': source.get('file_id', ''),
                        'title': source.get('title', ''),
                        'abstract': source.get('abstract', ''),
                        'year': source.get('year', ''),
                        'publisher': source.get('publisher', ''),
                        'author': source.get('author', ''),
                        'content': source.get('content', '')
                    })
                else:
                    logger.error(f"Document with file_id {file_id} not found in index 'document_metadata'")
            except Exception as e:
                logger.error(f"Error retrieving document with file_id {file_id}: {e}")

        if not all_refs:
            return jsonify({'file_ids': all_file_ids}), 200

        # å¦‚æœåªæœ‰ä¸€ä¸ªå¼•ç”¨å¯¹ï¼Œç›´æ¥è¿”å›ç»“æœ
        if len(all_refs) == 1:
            return jsonify({
                'status': 'success',
                'file_ids': [all_refs[0]['file_id']],
                'details': all_refs
            }), 200

        # ä½¿ç”¨é‡æ’æ¨¡å‹è¿›è¡Œé‡æ’å¹¶å½’ä¸€åŒ–å¾—åˆ†
        ref_pairs = [[query, ref['abstract']] for ref in all_refs]  # ä¸ºæ¯ä¸ªå‚è€ƒæ–‡æ¡£ä¸æŸ¥è¯¢ç»„åˆæˆå¯¹
        scores = reranker.compute_score(ref_pairs, normalize=True)  # è®¡ç®—æ¯å¯¹çš„å¾—åˆ†å¹¶å½’ä¸€åŒ–
        if not isinstance(scores, list):
            logger.error(f"Scores should be a list but got {type(scores)}: {scores}")
            return jsonify({'status': 'error', 'message': 'Invalid score format'}), 500

        # æ ¹æ®å¾—åˆ†æ’åºï¼Œå¹¶é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¼•ç”¨
        sorted_refs = sorted(zip(all_refs, scores), key=lambda x: x[1], reverse=True)
        top_refs = [ref for ref, _ in sorted_refs[:ref_num]]  # å‡è®¾æ‚¨åªéœ€è¦å‰ref_numä¸ªæœ€ç›¸å…³çš„å¼•ç”¨
        top_scores = [score for _, score in sorted_refs[:ref_num]]

        # æ‰“å°å‡ºè¿™äº›åˆ†æ•°
        logger.info(f"Top scores: {top_scores}")

        # è¿”å›ç»“æœ
        return jsonify({
            'status': 'success',
            'file_ids': [ref['file_id'] for ref in top_refs],
            'details': top_refs
        }), 200

    except Exception as e:
        logger.error(f"Error in ST_search_file: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/title_rewrite', methods=['POST'])
def title_generation():
    data = request.json
    session_id = data.get('sessionId')
    question = data.get('question')
    answer = data.get('answer')
    try:
        content = f"é—®é¢˜ï¼š{question}\nå›ç­”ï¼š{answer}\n"
        final_prompt = prompt_builder.generate_title_prompt(content)
        title = large_model_service.get_answer_from_Tyqwen(final_prompt)
        logger.info(f"Title generated: {title}")
        return jsonify({
            "code": 200,  # çŠ¶æ€ç 
            "data": {
                "label": title
            }
        })
    except Exception as e:
        logger.error(f'Error in session {session_id} during title_generation: {e}')
        return jsonify({
            "code": 500,  # æœåŠ¡å™¨å†…éƒ¨é”™è¯¯çŠ¶æ€ç 
            "data": {
                "label": question
            }
        })


@app.route('/SnoopIE', methods=['POST'])
def get_snoopIE_ans():
    data = request.json
    query = data.get('query')
    data = {
        "messages": [
            {"role": "user", "content": query},
        ]
    }
    # å‘é€POSTè¯·æ±‚åˆ°æ¥å£
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        "Content-Type": "application/json"
    }

    # å°†æ•°æ®è½¬æ¢ä¸ºJSONæ ¼å¼
    json_data = json.dumps(data)

    # å‘é€POSTè¯·æ±‚
    response = requests.post(api_url, headers=headers, data=json_data)
    # æ£€æŸ¥å“åº”çŠ¶æ€ç 
    if response.status_code == 200:
        # è·å–å“åº”æ•°æ®
        response_data = response.json()
        # æå–æ¨¡å‹å›ç­”çš„contentå­—æ®µ
        result_data = response_data['choices'][0]['message']['content']
        print(f"æ¨¡å‹å›ç­”: {result_data}")
        return result_data
    else:
        print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        return None


@app.route('/api/ST_get_answer_by_file_id', methods=['POST'])
def ST_answer_question_by_file_id():
    try:
        # åˆå§‹åŒ–é‡æ’æ¨¡å‹
        reranker = FlagReranker(rerank_model_path, use_fp16=True)
        # æ”¶é›†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æœ¬ç‰‡æ®µ
        metadata_refs = []
        all_refs = []
        # è¯»å–è¯·æ±‚å‚æ•°
        data = request.json
        assistant_id = data.get('assistant_id')
        session_id = data.get('session_id')
        token = data.get('token')
        query = data.get('query')
        file_id_list = data.get('file_id')
        ref_num = data.get('ref_num', 3)
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0)

        if not assistant_id or not query or not file_id_list:
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        # æ£€æŸ¥é—®é¢˜æ˜¯å¦åœ¨é¢„å®šä¹‰çš„é—®ç­”ä¸­
        predefined_answer = config.predefined_qa.get(query)
        if predefined_answer:
            if isinstance(predefined_answer, str):
                return jsonify({'answer': predefined_answer, 'matches': []}), 200
            elif isinstance(predefined_answer, dict) and 'answer' in predefined_answer and 'matches' in predefined_answer:
                return jsonify({
                    'answer': predefined_answer['answer'],
                    'matches': predefined_answer['matches']
                }), 200

        # æœç´¢åªåœ¨ç»™å®šçš„ file_id çš„æ–‡ä»¶å†…å®¹ä¸­è¿›è¡Œ
        bm25_refs = es_handler.ST_search_bm25(assistant_id, query, ref_num, file_id_list=file_id_list)
        embed_refs = es_handler.ST_search_embed(assistant_id, query, ref_num, file_id_list=file_id_list)
        all_refs = bm25_refs + embed_refs

        if not all_refs:
            def generate():
                full_answer = "æ‚¨çš„é—®é¢˜æ²¡æœ‰åœ¨æ–‡çŒ®èµ„æ–™ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œæ­£åœ¨ä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†åº“ä¸ºæ‚¨è§£ç­”ï¼š"
                Prompt = [{'role': 'user', 'content': query}]
                ans_generator = large_model_service.get_answer_from_Tyqwen_stream(Prompt, top_p=top_p, temperature=temperature)
                for chunk in ans_generator:
                    full_answer += chunk
                    data_stream = json.dumps({'matches': [], 'answer': full_answer}, ensure_ascii=False)
                    yield data_stream + '\n'

            return Response(stream_with_context(generate()), content_type='application/json; charset=utf-8')

        # è·å–æ‰€æœ‰åŒ¹é…æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        for file_id in file_id_list:
            try:
                doc = es_handler.es.get(index='document_metadata', id=file_id)
                if doc:
                    source = doc['_source']
                    metadata_refs.append({
                        'file_id': str(file_id),
                        'title': source.get('title', ''),
                        'year': source.get('year', ''),
                        'publisher': source.get('publisher', ''),
                        'author': source.get('author', ''),
                    })
            except Exception as e:
                logger.error(f"æ²¡æœ‰æ‰¾åˆ°è¯¥æ–‡çŒ®çš„æ–‡æœ¬ä¿¡æ¯ {file_id}: {e}")

        logger.info(f"Metadata references collected: {metadata_refs}")

        # ä½¿ç”¨é‡æ’æ¨¡å‹è¿›è¡Œé‡æ’å¹¶å½’ä¸€åŒ–å¾—åˆ†
        ref_pairs = [[query, ref['text']] for ref in all_refs]
        scores = reranker.compute_score(ref_pairs, normalize=True)
        sorted_refs = sorted(zip(all_refs, scores), key=lambda x: x[1], reverse=True)
        top_list = sorted_refs[:ref_num]
        top_scores = [score for _, score in sorted_refs[:5]]
        logger.info(f"Top scores: {top_scores}")
        top_refs = [ref for ref, score in top_list]

        # æ£€æŸ¥ top_refs ä¸­çš„ file_id
        for ref in top_refs:
            if 'file_id' not in ref:
                logger.error(f"Missing file_id in reference: {ref}")

        # å°†å…ƒæ•°æ®ä¸æ–‡æœ¬ç‰‡æ®µä¿¡æ¯åˆå¹¶
        merged_refs = []
        for ref in top_refs:
            file_id = str(ref.get('file_id'))  # ç¡®ä¿ file_id æ˜¯å­—ç¬¦ä¸²ç±»å‹
            if not file_id:
                logger.error(f"No file_id found in reference: {ref}")
                continue
            # æ‰¾åˆ°å¯¹åº”çš„å…ƒæ•°æ®
            metadata = next((metadata for metadata in metadata_refs if metadata['file_id'] == file_id), None)
            if metadata:
                # åˆå¹¶å…ƒæ•°æ®å’Œæ–‡æœ¬ç‰‡æ®µä¿¡æ¯
                merged_ref = {**ref, **metadata}
                merged_refs.append(merged_ref)
            else:
                logger.warning(f"No metadata found for file_id {file_id}")

        # è·å–å†å²å¯¹è¯å†…å®¹
        history = prompt_builder.get_history(session_id, token)
        # logger.info(f"Session ID: {session_id}")
        # logger.info(f"Token: {token}")
        logger.info(f"History: {history}")
        # åˆå§‹åŒ–é»˜è®¤çš„promptå’Œmatches
        prompt = None
        matches = []

        # è·å–ä¹‹å‰çš„æŸ¥è¯¢å’Œå¯¹åº”çš„ç»“æœ
        previous_queries = []
        if history:
            for item in history:
                if 'question' in item and 'documents' in item:
                    previous_queries.append((item['question'], item['documents']))

        # æ£€æŸ¥æœ€é«˜åˆ†æ•°æ˜¯å¦ä½äº0.3
        if top_scores[0] < 0.3:
            logger.info("é—®é¢˜ä¸æŸ¥è¯¢æ–‡çŒ®æ— å…³ï¼Œå±äºç»§ç»­æé—®")
            last_query = None
            last_matches = []

            # æ‰¾åˆ°ä¸å½“å‰é—®é¢˜ç›¸å…³çš„ä¸Šä¸€æ¬¡æœ‰æ•ˆæŸ¥è¯¢
            if previous_queries:
                for prev_query, prev_matches in reversed(previous_queries):
                    if prev_matches:
                        last_query = prev_query
                        last_matches = prev_matches
                        break

            if last_query and last_matches:
                prompt = prompt_builder.generate_ST_answer_prompt(query, last_matches, history)
                matches = [{
                    'text': doc['text'],
                    'file_id': doc['file_id'],
                    'file_name': doc.get('file_name', ''),
                    'score': doc.get('score', 0)
                } for doc in last_matches]
                logger.info(f"ä½¿ç”¨äº†generate_ST_answer_promptï¼Œç”Ÿæˆçš„promptï¼š{prompt}")
            else:
                prompt = prompt_builder.generate_answer_prompt_un_refs(query, history)
                matches = []
        else:
            prompt = prompt_builder.generate_ST_answer_prompt(query, merged_refs, history)
            matches = [{
                'text': ref['text'],
                'file_id': ref['file_id'],
                'file_name': ref['file_name'],
                'score': ref['score'],
                'rerank_score': score
            } for ref, score in top_list]
            logger.info(f"ä½¿ç”¨äº†generate_ST_answer_promptï¼Œç”Ÿæˆçš„promptï¼š{prompt}")

        if llm == 'qwen':
            def generate():
                full_answer = ""
                ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
                for chunk in ans_generator:
                    full_answer += chunk
                    data_stream = json.dumps({'matches': matches, 'answer': full_answer}, ensure_ascii=False)
                    yield data_stream + '\n'

                logger.info(f"é—®é¢˜ï¼š{query}")
                logger.info(f"ç”Ÿæˆçš„ç­”æ¡ˆï¼š{full_answer}")

            # logger.info(f"å‘½ä¸­æ–‡æ¡£ï¼š{matches}")
            return Response(stream_with_context(generate()), content_type='application/json; charset=utf-8')

        else:
            return jsonify({'error': 'æœªçŸ¥çš„å¤§æ¨¡å‹æœåŠ¡'}), 400

    except Exception as e:
        logger.error(f"Error in answer_question: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/shutdown', methods=['POST'])
def shutdown():
    large_model_service.shutdown()
    return jsonify({'status': 'success', 'message': 'Service shutdown'})


@app.route('/api/ST_OCR', methods=['POST'])
def ocr_indexing():
    try:
        data = request.json

        # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åŒ…å«docså­—æ®µ
        if 'docs' not in data:
            raise KeyError("'docs'å­—æ®µä¸å­˜åœ¨")

        documents = data['docs']
        # ç´¢å¼•åç§°
        index_name = 'st_ocr'

        # ç´¢å¼•mapping
        mappings = {
            "properties": {
                "AB": {"type": "text", "analyzer": "standard"},
                "CT": {"type": "text", "analyzer": "standard"},
                "Id": {"type": "keyword"},
                "Issue_F": {"type": "keyword"},
                "KW": {"type": "keyword"},
                "JTI": {"type": "keyword"},
                "Pid": {"type": "keyword"},
                "Piid": {"type": "keyword"},
                "TI": {"type": "text", "analyzer": "standard"},
                "Year": {"type": "integer"},
                "AB_embed": {"type": "dense_vector", "dims": 1024},
                "CT_embed": {"type": "dense_vector", "dims": 1024}
            }
        }

        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç´¢å¼•
        if not es_handler.index_exists(index_name):
            logger.info(f"åˆ›å»ºç´¢å¼• {index_name}")
            es_handler.es.indices.create(index=index_name, mappings=mappings)

        for document in documents:
            doc_id = document.get('Id')
            doc_abstract = document.get('AB', None)
            doc_content = document.get('CT', '')

            # å¦‚æœæ‘˜è¦ä¸ºç©ºï¼Œç”Ÿæˆæ‘˜è¦
            if not doc_abstract or not any(doc_abstract):
                logger.info("æ‘˜è¦ä¸ºç©ºï¼Œç”Ÿæˆæ‘˜è¦")
                abstract_prompt = prompt_builder.generate_abstract_prompt(doc_content)
                doc_abstract = large_model_service.get_answer_from_Tyqwen(abstract_prompt)  # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ‘˜è¦
                logger.info(f"ç”Ÿæˆæ‘˜è¦æˆåŠŸï¼Œæ–‡æ¡£ID: {doc_id}, æ‘˜è¦: {doc_abstract}")
                document['AB'] = [doc_abstract]

            # è®¡ç®—åµŒå…¥
            ab_embed = es_handler.cal_passage_embed(doc_abstract)
            ct_embed = es_handler.cal_passage_embed(doc_content)

            # æ·»åŠ åµŒå…¥åˆ°æ–‡æ¡£ä¸­
            document['AB_embed'] = ab_embed
            document['CT_embed'] = ct_embed

            es_handler.es.index(index=index_name, id=doc_id, document=document)
            logger.info(f"æ’å…¥æ–‡æ¡£ {doc_id} åˆ°ç´¢å¼• {index_name}")

        logger.info(f"ç´¢å¼• {index_name} åˆ›å»ºå¹¶æ’å…¥æ•°æ®æˆåŠŸ")
        return jsonify({'status': 'success'}), 200

    except KeyError as e:
        logger.error(f"ç´¢å¼•åˆ›å»ºæˆ–æ’å…¥æ•°æ®å¤±è´¥: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 400

    except Exception as e:
        logger.error(f"ç´¢å¼•åˆ›å»ºæˆ–æ’å…¥æ•°æ®å¤±è´¥: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/Canvas_chatpdf', methods=['POST'])
def Canvas_chatpdf():
    try:
        # è¯»å–è¯·æ±‚å‚æ•°
        data = request.json
        query = data.get('query')
        session_id = data.get('session_id')
        token = data.get('token')
        file_id = data.get('file_id')
        assistant_id = data.get('assistant_id')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0.1)
        user_info = data.get('userInfo', {})

        if not query or not file_id:
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        # è§£æç”¨æˆ·èº«ä»½ä¿¡æ¯
        outer_origin = user_info.get('outerOrigin', 'canvas')  # é»˜è®¤ä¸ºcanvas
        outer_user_name = user_info.get('outerUserName', 'ç”¨æˆ·')  # é»˜è®¤å€¼
        outer_user_role = user_info.get('outerUserRole', '1')  # é»˜è®¤å€¼ä¸ºå­¦ç”Ÿ

        # ç”Ÿæˆç”¨æˆ·èº«ä»½ä¸Šä¸‹æ–‡
        if outer_user_role == '2':
            role_description = "è€å¸ˆ"
        else:
            role_description = "å­¦ç”Ÿ"

        user_context = f"æ‚¨å¥½ï¼Œæˆ‘æ˜¯{outer_user_name}{role_description}ã€‚"

        try:
            history = prompt_builder.get_history(session_id, token)
        except Exception as e:
            history = []  # æˆ–è€…ä½ å¯ä»¥é€‰æ‹©è¿”å›é»˜è®¤çš„ç©ºå†å²
            logger.warning(f"è·å–å†å²å¯¹è¯å†…å®¹æ—¶å‡ºé”™: {e}")

        # ä» Elasticsearch ä¸­è·å–å…¨æ–‡å†…å®¹
        all_content = ""
        try:
            all_content = es_handler.get_full_text_by_file_id(assistant_id.strip(), file_id.strip())
            if all_content:
                logger.info("æ£€ç´¢åˆ°å…¨æ–‡å†…å®¹")
            else:
                logger.info(f"æœªèƒ½æ£€ç´¢åˆ°å…¨æ–‡å†…å®¹ for Assistant ID: {assistant_id}, File ID: {file_id}")
        except Exception as e:
            logger.error(f"æ£€ç´¢æ–‡æœ¬å†…å®¹æ—¶å‡ºé”™ {file_id}: {e}")

        if not all_content:
            def generate():
                full_answer = "æ‚¨çš„é—®é¢˜æ²¡æœ‰åœ¨æ–‡çŒ®èµ„æ–™ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œæ­£åœ¨ä½¿ç”¨é¢„è®­ç»ƒçŸ¥è¯†åº“ä¸ºæ‚¨è§£ç­”ï¼š"
                prompt = [{'role': 'user', 'content': query}]
                ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p,
                                                                                  temperature=temperature)
                for chunk in ans_generator:
                    full_answer += chunk
                    data_stream = json.dumps({'matches': [], 'answer': full_answer}, ensure_ascii=False)
                    yield data_stream + '\n'

            return Response(stream_with_context(generate()), content_type='application/json; charset=utf-8')

        # ç”Ÿæˆ prompt
        prompt_messages = prompt_builder.generate_canvas_prompt(query, all_content, history, user_context)
        logger.info(f"Prompt: {prompt_messages}")

        if llm == 'qwen':
            def generate():
                full_answer = ""
                ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt_messages, top_p=top_p, temperature=temperature)
                for chunk in ans_generator:
                    full_answer += chunk
                    data_stream = json.dumps({'answer': full_answer}, ensure_ascii=False)
                    yield data_stream + '\n'

                logger.info(f"é—®é¢˜ï¼š{query}")
                logger.info(f"ç”Ÿæˆçš„ç­”æ¡ˆï¼š{full_answer}")

            return Response(stream_with_context(generate()), content_type='application/json; charset=utf-8')
        if llm == 'deepseek':
            def generate():
                full_answer = ""
                ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt_messages, top_p=top_p, temperature=temperature)
                for chunk in ans_generator:
                    full_answer += chunk
                    data_stream = json.dumps({'answer': full_answer}, ensure_ascii=False)
                    yield data_stream + '\n'

                logger.info(f"é—®é¢˜ï¼š{query}")
                logger.info(f"ç”Ÿæˆçš„ç­”æ¡ˆï¼š{full_answer}")

            return Response(stream_with_context(generate()), content_type='application/json; charset=utf-8')

        else:
            return jsonify({'error': 'æœªçŸ¥çš„å¤§æ¨¡å‹æœåŠ¡'}), 400

    except Exception as e:
        logger.error(f"Error in answer_question: {e}")
        return jsonify({'error': str(e)}), 500


# å®šä¹‰ Flask è·¯ç”±
@app.route('/api/web_search', methods=['POST'])
def web_search():
    data = request.json
    query = data.get('query')

    if not query:
        return jsonify({"error": "Query is required"}), 400

    # è°ƒç”¨ large_model_service çš„æ–¹æ³•
    result = large_model_service.web_search_glm4(query)

    return jsonify(result)


def start_background_threads():
    threads = [threading.Thread(target=_thread_index_func, args=(i == 0,)) for i in range(32)]
    for t in threads:
        t.daemon = True  # å°†çº¿ç¨‹è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
        t.start()


@app.route('/api/canvas_qa', methods=['POST'])
def canvas_chatpdf():
    try:
        # è¯»å–è¯·æ±‚å‚æ•°
        data = request.json
        # æ‰“å°åŸå§‹è¯·æ±‚æ•°æ®
        query = data.get('query')
        session_id = data.get('session_id')
        download_path = data.get('download_path')
        file_name = data.get('file_name')
        file_id = data.get('file_id')
        token = data.get('token')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.7)
        temperature = data.get('temperature', 0.1)
        user_info = data.get('userInfo', {})

        if not query or not download_path:
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        # è§£æç”¨æˆ·èº«ä»½ä¿¡æ¯
        outer_origin = user_info.get('outerOrigin', 'canvas')  # é»˜è®¤ä¸ºcanvas
        outer_user_name = user_info.get('outerUserName', 'ç”¨æˆ·')  # é»˜è®¤å€¼
        outer_user_role = user_info.get('outerUserRole', '1')  # é»˜è®¤å€¼ä¸ºå­¦ç”Ÿ

        # ç”Ÿæˆç”¨æˆ·èº«ä»½ä¸Šä¸‹æ–‡
        if outer_user_role == '2':
            role_description = "è€å¸ˆ"
        else:
            role_description = "å­¦ç”Ÿ"

        user_context = f"æ‚¨å¥½ï¼Œæˆ‘æ˜¯{outer_user_name}{role_description}ã€‚"

        try:
            history = prompt_builder.get_history(session_id, token)
        except Exception as e:
            history = []  # æˆ–è€…ä½ å¯ä»¥é€‰æ‹©è¿”å›é»˜è®¤çš„ç©ºå†å²
            logger.warning(f"è·å–å†å²å¯¹è¯å†…å®¹æ—¶å‡ºé”™: {e}")

        logger.info("å¼€å§‹ä¸‹è½½æ–‡ä»¶å¹¶å¤„ç†: %sï¼Œæ–‡ä»¶è·¯å¾„ï¼š%s", file_name, download_path)
        file_path = file_manager.download_pdf(download_path, file_id)
        full_text = file_manager.process_Canvas_file(file_path, file_name)

        # ç”Ÿæˆ prompt
        prompt_messages = prompt_builder.generate_canvas_prompt(query, full_text, history, user_context)
        logger.info(f"Prompt: {prompt_messages}")

        if llm == 'qwen':
            def generate():
                full_answer = ""
                ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt_messages, top_p=top_p, temperature=temperature)
                for chunk in ans_generator:
                    full_answer += chunk
                    data_stream = json.dumps({'answer': full_answer}, ensure_ascii=False)
                    yield data_stream + '\n'

            return Response(stream_with_context(generate()), content_type='application/json; charset=utf-8')
        else:
            return jsonify({'error': 'æœªçŸ¥çš„å¤§æ¨¡å‹æœåŠ¡'}), 400

    except Exception as e:
        logger.error(f"Error in answer_question: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/image2text_stream', methods=['POST'])
def image2text_stream():
    try:
        data = request.json
        query = data.get('query')
        llm = data.get('llm', 'internvl')  # é»˜è®¤ä½¿ç”¨ internvl2.5-latest
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0)

        # åˆ¤æ–­ç”¨æˆ·è¾“å…¥æ˜¯æ–‡å­—è¿˜æ˜¯å›¾ç‰‡
        messages = []
        if isinstance(query, list):
            for item in query:
                message_content = []

                # å¦‚æœæœ‰æ–‡å­—éƒ¨åˆ†
                if 'text' in item and isinstance(item['text'], str):
                    message_content.append({"type": "text", "text": item['text']})

                # å¦‚æœæœ‰å›¾ç‰‡éƒ¨åˆ†
                if 'image' in item and isinstance(item['image'], str):
                    # å¦‚æœåªæœ‰å›¾ç‰‡ï¼Œè‡ªåŠ¨å¡«å…… text
                    if not any(c['type'] == 'text' for c in message_content):
                        message_content.append({"type": "text", "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡"})
                    message_content.append({
                        "type": "image_url",
                        "image_url": {"url": item['image']}
                    })

                # å¦‚æœ message_content åªåŒ…å«æ–‡å­—è€Œæ²¡æœ‰å›¾ç‰‡
                if message_content and len(message_content) == 1 and 'text' in message_content[0]:
                    messages.append({
                        "role": "user",
                        "content": message_content[0]["text"]
                    })
                elif message_content:
                    # å¦‚æœæœ‰å›¾ç‰‡å’Œæ–‡å­—ï¼Œè¿”å›æ•°ç»„å½¢å¼
                    messages.append({
                        "role": "user",
                        "content": message_content
                    })
                else:
                    return jsonify({"error": "æ¯ä¸ªæ¡ç›®å¿…é¡»åŒ…å«æœ‰æ•ˆçš„ text æˆ– image å­—æ®µ"}), 400
        else:
            return jsonify({"error": "query å¿…é¡»æ˜¯ä¸€ä¸ªåˆ—è¡¨"}), 400

        logger.info(f"æ„å»ºçš„æç¤ºè¯: {messages}")

        # è°ƒç”¨æ¨¡å‹è·å–å›ç­”å¹¶è¿”å›æµå¼å“åº”
        def generate_stream_response():
            # ä½¿ç”¨ä½ çš„æ–¹æ³•è·å–æµå¼è¾“å‡º
            for part in large_model_service.get_answer_from_internvl_stream(messages, top_p, temperature):
                yield part  # è¿”å›æµå¼å†…å®¹

        return Response(generate_stream_response(), content_type='application/json; charset=utf-8')

    except Exception as e:
        logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return jsonify({"error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯", "details": str(e)}), 500


@app.route('/api/beautify_prompt', methods=['POST'])
def prompt_rewrite():
    try:
        data = request.json
        query = data.get('query')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0.6)

        if llm == 'qwen':
            prompt = prompt_builder.generate_prompt_for_qwen(query)
            ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')
        if llm == 'deepseek':
            prompt = prompt_builder.generate_prompt_for_deepseek(query)
            ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

    except Exception as e:
        logger.error(f"Error in prompt_rewrite: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/Text_polish', methods=['POST'])
def polish_text():
    try:
        # Get the question from the request payload
        data = request.get_json()
        question = data.get('question', '')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0.6)
        if not question :
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        prompt = PromptBuilder.generate_polish_prompt(question)

        if llm == 'qwen':
            ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

        if llm == 'deepseek':
            ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': f'æ–‡æœ¬æ¶¦è‰²æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        }), 500


@app.route('/api/text_expansion', methods=['POST'])
def expand_text():
    try:
        # Get the question from the request payload
        data = request.get_json()
        question = data.get('question', '')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0.6)
        if not question :
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        prompt = PromptBuilder.generate_expand_prompt(question)

        if llm == 'qwen':
            ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

        if llm == 'deepseek':
            ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': f'æ–‡æœ¬æ‰©å†™æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        }), 500


@app.route('/api/text_translation', methods=['POST'])
def transaltion_text():
    try:
        # Get the question from the request payload
        data = request.get_json()
        question = data.get('question', '')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0.4)
        if not question :
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        prompt = PromptBuilder.generate_translation_prompt(question)

        if llm == 'qwen':
            ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

        if llm == 'deepseek':
            ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': f'æ–‡æœ¬æ‰©å†™æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        }), 500


@app.route('/api/polite_language', methods=['POST'])
def polite_text():
    try:
        # Get the question from the request payload
        data = request.get_json()
        question = data.get('question', '')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0.7)
        style = data.get('style', 'å•†åŠ¡ç¤¼ä»ª')
        if not question :
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        prompt = PromptBuilder.generate_politeness_prompt(question, style)

        if llm == 'qwen':
            ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

        if llm == 'deepseek':
            ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': f'ç¤¼è²ŒåŒ–æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        }), 500


@app.route('/api/how_to_say_no', methods=['POST'])
def refuse_text():
    try:
        # Get the question from the request payload
        data = request.get_json()
        question = data.get('question', '')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0.7)
        if not question :
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        prompt = PromptBuilder.generate_refusal_prompt(question)

        if llm == 'qwen':
            ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

        if llm == 'deepseek':
            ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': f'æ‹’ç»åˆ«äººæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        }), 500


@app.route('/api/write_email', methods=['POST'])
def email_text():
    try:
        # Get the question from the request payload
        data = request.get_json()
        question = data.get('question', '')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0.7)
        if not question :
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        prompt = PromptBuilder.generate_email_prompt(question)

        if llm == 'qwen':
            ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

        if llm == 'deepseek':
            ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': f'ç”Ÿæˆé‚®ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        }), 500


@app.route('/api/poem_writer', methods=['POST'])
def poem_text():
    try:
        # Get the question from the request payload
        data = request.get_json()
        question = data.get('question', '')
        llm = data.get('llm', 'qwen').lower()
        top_p = data.get('top_p', 0.8)
        temperature = data.get('temperature', 0.7)
        poetry_style = data.get('poetry_style', 'ä¿³å¥')
        if not question :
            return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

        prompt = PromptBuilder.generate_poem_prompt(poetry_style, question)

        if llm == 'qwen':
            ans_generator = large_model_service.get_answer_from_Tyqwen_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

        if llm == 'deepseek':
            ans_generator = large_model_service.get_answer_from_deepseek_stream(prompt, top_p=top_p, temperature=temperature)
            return Response(ans_generator, content_type='text/plain; charset=utf-8')

    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': f'å†™è¯—æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        }), 500


# ç¡®ä¿çº¿ç¨‹åœ¨ Flask åº”ç”¨å¯åŠ¨å‰å°±å¯åŠ¨
start_background_threads()


if __name__ == '__main__':
    # åœ¨æœ¬åœ°è°ƒè¯•æ—¶å¯ä»¥ç»§ç»­ä½¿ç”¨ Flask å†…ç½®æœåŠ¡å™¨
    app.run(host='0.0.0.0', port=5777, debug=False)



